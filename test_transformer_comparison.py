import torch
import json
from pathlib import Path
from cs336_basics.transformer import TransformerBlock
from tests.adapters import run_transformer_block

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®å’Œæƒé‡"""
    fixtures_path = Path("tests/fixtures")
    
    # åŠ è½½æ¨¡å‹æƒé‡å’Œé…ç½®
    state_dict = torch.load(fixtures_path / "ts_tests" / "model.pt", map_location="cpu")
    config = json.load(open(fixtures_path / "ts_tests" / "model_config.json"))
    state_dict = {k.replace("_orig_mod.", ""): v for k, v in state_dict.items()}
    
    return state_dict, config

def extract_layer_weights(state_dict, layer_idx=0):
    """æå–æŒ‡å®šå±‚çš„æƒé‡"""
    layer_weights = {}
    prefix = f"layers.{layer_idx}."
    
    for key, value in state_dict.items():
        if key.startswith(prefix):
            # ç§»é™¤å±‚å‰ç¼€ï¼Œä¿ç•™ç›¸å¯¹è·¯å¾„
            relative_key = key[len(prefix):]
            layer_weights[relative_key] = value
    
    return layer_weights

def test_transformer_block_comparison():
    """å¯¹æ¯”å®˜æ–¹å®ç°å’Œè‡ªå®šä¹‰å®ç°"""
    print("=== Transformer Block å¯¹æ¯”æµ‹è¯• ===")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    state_dict, config = load_test_data()
    
    # æå–é…ç½®å‚æ•°
    d_model = config['d_model']
    num_heads = config['num_heads']
    d_ff = config['d_ff']
    max_seq_len = config['context_length']
    theta = config['rope_theta']
    
    print(f"é…ç½®å‚æ•°:")
    print(f"  d_model: {d_model}")
    print(f"  num_heads: {num_heads}")
    print(f"  d_ff: {d_ff}")
    print(f"  max_seq_len: {max_seq_len}")
    print(f"  theta: {theta}")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    torch.manual_seed(42)
    batch_size = 2
    seq_len = 8  # ä½¿ç”¨è¾ƒçŸ­çš„åºåˆ—é•¿åº¦
    in_features = torch.randn(batch_size, seq_len, d_model)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {in_features.shape}")
    
    # æå–ç¬¬0å±‚çš„æƒé‡
    layer_weights = extract_layer_weights(state_dict, layer_idx=0)
    
    print(f"\næå–çš„æƒé‡é”®:")
    for key in sorted(layer_weights.keys()):
        print(f"  {key}: {layer_weights[key].shape}")
    
    # æµ‹è¯•å®˜æ–¹å®ç°
    print(f"\n--- å®˜æ–¹å®ç°æµ‹è¯• ---")
    try:
        # æ„å»ºå®Œæ•´çš„æƒé‡å­—å…¸ï¼ˆå®˜æ–¹adapteréœ€è¦çš„æ ¼å¼ï¼‰
        full_weights = {}
        for key, value in layer_weights.items():
            full_weights[key] = value
        
        official_output = run_transformer_block(
            d_model=d_model,
            num_heads=num_heads,
            d_ff=d_ff,
            max_seq_len=max_seq_len,
            theta=theta,
            weights=full_weights,
            in_features=in_features
        )
        
        print(f"âœ… å®˜æ–¹å®ç°æˆåŠŸ")
        print(f"è¾“å‡ºå½¢çŠ¶: {official_output.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{official_output.min().item():.4f}, {official_output.max().item():.4f}]")
        print(f"è¾“å‡ºå‡å€¼: {official_output.mean().item():.4f}")
        print(f"è¾“å‡ºæ ‡å‡†å·®: {official_output.std().item():.4f}")
        
    except Exception as e:
        print(f"âŒ å®˜æ–¹å®ç°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æµ‹è¯•è‡ªå®šä¹‰å®ç°
    print(f"\n--- è‡ªå®šä¹‰å®ç°æµ‹è¯• ---")
    try:
        # ä¿®æ­£æƒé‡é”®åä»¥åŒ¹é…è‡ªå®šä¹‰å®ç°çš„æœŸæœ›æ ¼å¼
        custom_weights = {}
        
        # æ³¨æ„åŠ›æƒé‡æ˜ å°„
        if 'attn.q_proj.weight' in layer_weights:
            custom_weights['attn.q_proj.weight'] = layer_weights['attn.q_proj.weight']
        if 'attn.k_proj.weight' in layer_weights:
            custom_weights['attn.k_proj.weight'] = layer_weights['attn.k_proj.weight']
        if 'attn.v_proj.weight' in layer_weights:
            custom_weights['attn.v_proj.weight'] = layer_weights['attn.v_proj.weight']
        if 'attn.output_proj.weight' in layer_weights:
            custom_weights['attn.output_proj.weight'] = layer_weights['attn.output_proj.weight']
        
        # FFNæƒé‡æ˜ å°„
        if 'ffn.w1.weight' in layer_weights:
            custom_weights['ffn.w1.weight'] = layer_weights['ffn.w1.weight']
        if 'ffn.w2.weight' in layer_weights:
            custom_weights['ffn.w2.weight'] = layer_weights['ffn.w2.weight']
        if 'ffn.w3.weight' in layer_weights:
            custom_weights['ffn.w3.weight'] = layer_weights['ffn.w3.weight']
        
        # å½’ä¸€åŒ–æƒé‡æ˜ å°„
        if 'ln1.weight' in layer_weights:
            custom_weights['ln1.weight'] = layer_weights['ln1.weight']
        if 'ln2.weight' in layer_weights:
            custom_weights['ln2.weight'] = layer_weights['ln2.weight']
        
        print(f"è‡ªå®šä¹‰æƒé‡é”®:")
        for key in sorted(custom_weights.keys()):
            print(f"  {key}: {custom_weights[key].shape}")
        
        custom_transformer = TransformerBlock(
            d_model=d_model,
            num_heads=num_heads,
            d_ff=d_ff,
            max_seq_len=max_seq_len,
            theta=theta,
            weights_dict=custom_weights
        )
        
        custom_output = custom_transformer(in_features)
        
        print(f"âœ… è‡ªå®šä¹‰å®ç°æˆåŠŸ")
        print(f"è¾“å‡ºå½¢çŠ¶: {custom_output.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{custom_output.min().item():.4f}, {custom_output.max().item():.4f}]")
        print(f"è¾“å‡ºå‡å€¼: {custom_output.mean().item():.4f}")
        print(f"è¾“å‡ºæ ‡å‡†å·®: {custom_output.std().item():.4f}")
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰å®ç°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # å¯¹æ¯”ç»“æœ
    print(f"\n--- ç»“æœå¯¹æ¯” ---")
    
    # å½¢çŠ¶å¯¹æ¯”
    shapes_match = official_output.shape == custom_output.shape
    print(f"å½¢çŠ¶åŒ¹é…: {shapes_match}")
    
    if shapes_match:
        # æ•°å€¼å¯¹æ¯”
        max_diff = torch.max(torch.abs(official_output - custom_output)).item()
        mean_diff = torch.mean(torch.abs(official_output - custom_output)).item()
        relative_diff = mean_diff / (torch.mean(torch.abs(official_output)).item() + 1e-8)
        
        print(f"æœ€å¤§ç»å¯¹å·®å¼‚: {max_diff:.6f}")
        print(f"å¹³å‡ç»å¯¹å·®å¼‚: {mean_diff:.6f}")
        print(f"ç›¸å¯¹å·®å¼‚: {relative_diff:.6f}")
        
        # åˆ¤æ–­æ˜¯å¦æ¥è¿‘
        tolerance = 1e-4
        is_close = torch.allclose(official_output, custom_output, atol=tolerance, rtol=tolerance)
        print(f"æ•°å€¼æ¥è¿‘ (tolerance={tolerance}): {is_close}")
        
        if not is_close:
            print(f"\nè¯¦ç»†å·®å¼‚åˆ†æ:")
            diff = torch.abs(official_output - custom_output)
            print(f"å·®å¼‚åˆ†å¸ƒ:")
            print(f"  æœ€å°å·®å¼‚: {diff.min().item():.6f}")
            print(f"  25%åˆ†ä½æ•°: {torch.quantile(diff, 0.25).item():.6f}")
            print(f"  ä¸­ä½æ•°: {torch.median(diff).item():.6f}")
            print(f"  75%åˆ†ä½æ•°: {torch.quantile(diff, 0.75).item():.6f}")
            print(f"  æœ€å¤§å·®å¼‚: {diff.max().item():.6f}")
            
            # æ‰¾å‡ºå·®å¼‚æœ€å¤§çš„ä½ç½®
            max_diff_idx = torch.argmax(diff)
            max_diff_pos = torch.unravel_index(max_diff_idx, diff.shape)
            print(f"æœ€å¤§å·®å¼‚ä½ç½®: {max_diff_pos}")
            print(f"  å®˜æ–¹å€¼: {official_output[max_diff_pos].item():.6f}")
            print(f"  è‡ªå®šä¹‰å€¼: {custom_output[max_diff_pos].item():.6f}")
    
    else:
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼Œæ— æ³•è¿›è¡Œæ•°å€¼å¯¹æ¯”")
        print(f"  å®˜æ–¹è¾“å‡ºå½¢çŠ¶: {official_output.shape}")
        print(f"  è‡ªå®šä¹‰è¾“å‡ºå½¢çŠ¶: {custom_output.shape}")

def test_without_weights():
    """æµ‹è¯•æ— æƒé‡æƒ…å†µä¸‹çš„å®ç°"""
    print(f"\n=== æ— æƒé‡æµ‹è¯• ===")
    
    # åŸºæœ¬å‚æ•°
    d_model = 64
    num_heads = 4
    d_ff = 128
    max_seq_len = 16
    theta = 10000.0
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    torch.manual_seed(42)
    batch_size = 2
    seq_len = 8
    in_features = torch.randn(batch_size, seq_len, d_model)
    
    try:
        # æµ‹è¯•è‡ªå®šä¹‰å®ç°ï¼ˆæ— æƒé‡ï¼‰
        custom_transformer = TransformerBlock(
            d_model=d_model,
            num_heads=num_heads,
            d_ff=d_ff,
            max_seq_len=max_seq_len,
            theta=theta,
            weights_dict=None
        )
        
        output = custom_transformer(in_features)
        
        print(f"âœ… æ— æƒé‡æµ‹è¯•æˆåŠŸ")
        print(f"è¾“å…¥å½¢çŠ¶: {in_features.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"å½¢çŠ¶åŒ¹é…: {output.shape == in_features.shape}")
        
        # æ£€æŸ¥æ¢¯åº¦æµ
        loss = output.sum()
        loss.backward()
        
        has_gradients = any(p.grad is not None for p in custom_transformer.parameters())
        print(f"æ¢¯åº¦è®¡ç®—æ­£å¸¸: {has_gradients}")
        
    except Exception as e:
        print(f"âŒ æ— æƒé‡æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_transformer_block_comparison()
    test_without_weights()
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")